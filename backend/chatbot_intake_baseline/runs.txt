cim_base_tuned.py:
Using device: cpu
============================================================
MEDICAL BASELINE
T5-Base + spaCy (80/10/10 Split)
Conversation -> Summary Evaluation
============================================================
Loading T5-Base model: t5-base
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
/Users/pranavcheraku/Personal/projects/er_comms_assistant/.medassist_venv/lib/python3.10/site-packages/spacy/language.py:2195: FutureWarning: Possible set union at position 6328
  deserializers["tokenizer"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]
Loaded SciSpaCy model: en_core_sci_sm
Loading medical dataset...
Loaded 30000 samples from HuggingFace dataset
Using sample of 1000 records for faster T5-Base training
Processing data: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 63823.73it/s]
Processed 1000 valid samples
Data Split (80/10/10):
  Train: 800 samples (80.0%)
  Dev: 100 samples (10.0%)
  Test: 100 samples (10.0%)
============================================================
STARTING T5-BASE FINE-TUNING
============================================================
Fine-tuning T5-Base on 800 samples...
Validating on 100 samples...
Training for 3 epochs...
Model: t5-base
Batch size: 8, Learning rate: 5e-05

Epoch 1/3
Training Epoch 1:   0%|                                                                                                                                          | 0/100 [00:00<?, ?it/sPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
Training Epoch 1: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [17:47<00:00, 10.67s/it, loss=1.0035]
  Epoch 1 Average Loss: 1.4023
  Epoch 1 Validation Loss: 0.6601

Epoch 2/3
Training Epoch 2: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [17:02<00:00, 10.23s/it, loss=0.6158]
  Epoch 2 Average Loss: 0.6490
  Epoch 2 Validation Loss: 0.5842

Epoch 3/3
Training Epoch 3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [15:27<00:00,  9.27s/it, loss=0.5861]
  Epoch 3 Average Loss: 0.5658
  Epoch 3 Validation Loss: 0.5525
T5-Base fine-tuning completed
Final average training loss: 0.8724
Fine-tuned T5-Base model saved to cim_base_tuned_results/model
============================================================
EVALUATING FINE-TUNED T5-BASE MODEL
Conversation -> Summary Evaluation
============================================================
Generating predictions for 100 samples...
Evaluating T5-Base: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [07:56<00:00,  4.76s/it]
Calculating ROUGE scores...
Calculating BERTScore...
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Detailed results saved to cim_base_tuned_results/detailed_results.csv
Metrics saved to cim_base_tuned_results/evaluation_metrics.json

============================================================
MEDICAL BASELINE EVALUATION RESULTS
============================================================

TRAINING RESULTS:
  Model: t5-base
  Final Training Loss: 0.8724
  Training Samples: 800
  Dev Samples: 100
  Epochs: 3

EVALUATION RESULTS:
  Test Samples: 100

  ROUGE SCORES:
    ROUGE-1: 0.1234 (±0.0518)
    ROUGE-2: 0.0634 (±0.0329)
    ROUGE-L: 0.0904 (±0.0345)

  BERTSCORE:
    Precision: 0.8675
    Recall: 0.7645
    F1: 0.8126

  SUMMARY STATISTICS:
    Avg Predicted Length: 33.9 words
    Avg Reference Length: 453.2 words
    Length Ratio: 0.075

Results saved to: cim_base_tuned_results









cim_base_pretrained.py
Using device: cpu
============================================================
MEDICAL BASELINE
Pretrained T5-Base + spaCy (80/10/10 Split)
Conversation -> Summary Evaluation
No Fine-tuning - Using Pretrained Model Only
============================================================
Loading pretrained T5-Base model: t5-base
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
/Users/pranavcheraku/Personal/projects/er_comms_assistant/.medassist_venv/lib/python3.10/site-packages/spacy/language.py:2195: FutureWarning: Possible set union at position 6328
  deserializers["tokenizer"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]
Loaded SciSpaCy model: en_core_sci_sm
Loading medical dataset...
Loaded 30000 samples from HuggingFace dataset
Using sample of 1000 records for faster evaluation
Processing data: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 62782.40it/s]
Processed 1000 valid samples
Data Split (80/10/10):
  Train: 800 samples (80.0%)
  Dev: 100 samples (10.0%)
  Test: 100 samples (10.0%)

Skipping training - using pretrained T5-Base model
============================================================
EVALUATING PRETRAINED T5-BASE MODEL
Conversation -> Summary Evaluation
============================================================
Generating predictions for 100 samples...
Evaluating Pretrained T5-Base: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [06:52<00:00,  4.12s/it]
Calculating ROUGE scores...
Calculating BERTScore...
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Detailed results saved to cim_base_pretrained_results/detailed_results.csv
Metrics saved to cim_base_pretrained_results/evaluation_metrics.json

============================================================
PRETRAINED MEDICAL BASELINE EVALUATION RESULTS
============================================================

MODEL INFORMATION:
  Model: t5-base
  Type: pretrained (no fine-tuning)
  Test Samples: 100

  ROUGE SCORES:
    ROUGE-1: 0.1062 (±0.0318)
    ROUGE-2: 0.0518 (±0.0252)
    ROUGE-L: 0.0788 (±0.0262)

  BERTSCORE:
    Precision: 0.8434
    Recall: 0.7381
    F1: 0.7871

  SUMMARY STATISTICS:
    Avg Predicted Length: 37.0 words
    Avg Reference Length: 453.2 words
    Length Ratio: 0.082

Results saved to: cim_base_pretrained_results
